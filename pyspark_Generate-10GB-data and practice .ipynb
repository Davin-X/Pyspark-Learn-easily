{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fadc3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===done===\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import random\n",
    "import string\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "random.seed(1999)\n",
    "\n",
    "letters = string.ascii_lowercase\n",
    "letters_upper = string.ascii_uppercase\n",
    "for _i in range(0, 10):\n",
    "    letters += letters\n",
    "\n",
    "for _i in range(0, 10):\n",
    "    letters += letters_upper\n",
    "\n",
    "\n",
    "def random_string(stringLength=10):\n",
    "    \"\"\"Generate a random string of fixed length \"\"\"\n",
    "    return ''.join(random.sample(letters, stringLength))\n",
    "\n",
    "print(\"===done===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54ab70d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"Products between {} and {}\".format(1, 75000000))\\nproduct_ids = [x for x in range(1, 75000000)]\\ndates = [\\'2020-07-01\\', \\'2020-07-02\\', \\'2020-07-03\\', \\'2020-07-04\\', \\'2020-07-05\\', \\'2020-07-06\\', \\'2020-07-07\\', \\'2020-07-08\\',\\n         \\'2020-07-09\\', \\'2020-07-10\\']\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print(\"Products between {} and {}\".format(1, 75000000))\n",
    "product_ids = [x for x in range(1, 75000000)]\n",
    "dates = ['2020-07-01', '2020-07-02', '2020-07-03', '2020-07-04', '2020-07-05', '2020-07-06', '2020-07-07', '2020-07-08',\n",
    "         '2020-07-09', '2020-07-10']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39f2fe1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "#   Generate sellers\n",
    "seller_ids = [x for x in range(1, 10)]\n",
    "sellers = [[0, \"seller_0\", 2500000]]\n",
    "for s in tqdm(seller_ids):\n",
    "    sellers.append([s, \"seller_{}\".format(s), random.randint(12000, 2000000)])\n",
    "#   Save dataframe\n",
    "df = pd.DataFrame(sellers)\n",
    "df.columns = [\"seller_id\", \"seller_name\", \"daily_target\"]\n",
    "df.to_csv(\"sellers.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8303710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#   Generate sales\\ntotal_rows = 500000\\nprod_zero = int(total_rows * 0.95)\\nprod_others = total_rows - prod_zero + 1\\ndf_array = [[\"order_id\", \"product_id\", \"seller_id\", \"date\", \"num_pieces_sold\", \"bill_raw_text\"]]\\nwith open(\\'sales.csv\\', \\'w\\', newline=\\'\\') as f:\\n    csvwriter = csv.writer(f)\\n    csvwriter.writerows(df_array)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#   Generate sales\n",
    "'''\n",
    "total_rows = 500000\n",
    "prod_zero = int(total_rows * 0.95)\n",
    "prod_others = total_rows - prod_zero + 1\n",
    "df_array = [[\"order_id\", \"product_id\", \"seller_id\", \"date\", \"num_pieces_sold\", \"bill_raw_text\"]]\n",
    "with open('sales.csv', 'w', newline='') as f:\n",
    "    csvwriter = csv.writer(f)\n",
    "    csvwriter.writerows(df_array)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2e9e780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#   Generate products\\nproducts = [[0, \"product_0\", 22]]\\nfor p in tqdm(product_ids):\\n    products.append([p, \"product_{}\".format(p), random.randint(1, 150)])\\n#   Save dataframe\\ndf = pd.DataFrame(products)\\ndf.columns = [\"product_id\", \"product_name\", \"price\"]\\ndf.to_csv(\"products.csv\", index=False)\\ndel df\\ndel products\\n\\nprint(\"====done====\")'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#   Generate products\n",
    "'''\n",
    "products = [[0, \"product_0\", 22]]\n",
    "for p in tqdm(product_ids):\n",
    "    products.append([p, \"product_{}\".format(p), random.randint(1, 150)])\n",
    "#   Save dataframe\n",
    "df = pd.DataFrame(products)\n",
    "df.columns = [\"product_id\", \"product_name\", \"price\"]\n",
    "df.to_csv(\"products.csv\", index=False)\n",
    "del df\n",
    "del products\n",
    "\n",
    "print(\"====done====\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5ddf273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'order_id = 0\\nfor i in tqdm(range(0, 40)):\\n    df_array = []\\n\\n    for i in range(0, prod_zero):\\n        order_id += 1\\n        df_array.append([order_id, 0, 0, random.choice(dates), random.randint(1, 100), random_string(500)])\\n\\n    with open(\\'sales.csv\\', \\'a\\', newline=\\'\\') as f:\\n        csvwriter = csv.writer(f)\\n        csvwriter.writerows(df_array)\\n\\n    df_array = []\\n    for i in range(0, prod_others):\\n        order_id += 1\\n        df_array.append(\\n            [order_id, random.choice(product_ids), random.choice(seller_ids), random.choice(dates),\\n             random.randint(1, 100), random_string(500)])\\n\\n    with open(\\'sales.csv\\', \\'a\\', newline=\\'\\') as f:\\n        csvwriter = csv.writer(f)\\n        csvwriter.writerows(df_array)\\n\\nprint(\"Done\")'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''order_id = 0\n",
    "for i in tqdm(range(0, 40)):\n",
    "    df_array = []\n",
    "\n",
    "    for i in range(0, prod_zero):\n",
    "        order_id += 1\n",
    "        df_array.append([order_id, 0, 0, random.choice(dates), random.randint(1, 100), random_string(500)])\n",
    "\n",
    "    with open('sales.csv', 'a', newline='') as f:\n",
    "        csvwriter = csv.writer(f)\n",
    "        csvwriter.writerows(df_array)\n",
    "\n",
    "    df_array = []\n",
    "    for i in range(0, prod_others):\n",
    "        order_id += 1\n",
    "        df_array.append(\n",
    "            [order_id, random.choice(product_ids), random.choice(seller_ids), random.choice(dates),\n",
    "             random.randint(1, 100), random_string(500)])\n",
    "\n",
    "    with open('sales.csv', 'a', newline='') as f:\n",
    "        csvwriter = csv.writer(f)\n",
    "        csvwriter.writerows(df_array)\n",
    "\n",
    "print(\"Done\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e32adc7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nspark = SparkSession.builder.master(\"local\").config(\"spark.sql.autoBroadcastJoinThreshold\", -1).appName(\"Exercise1\").getOrCreate()\\n\\nproducts = spark.read.csv(\"file:////C:\\\\Users\\\\dev30\\\\Documents\\\\pyspark_practice\\\\products.csv\", header=True, mode=\"DROPMALFORMED\")\\n\\n#products.show()\\n\\nproducts.write.parquet(\"file:////C:\\\\Users\\\\dev30\\\\Documents\\\\pyspark_practice\\\\products_parquet\", mode=\"overwrite\")\\n\\nsales = spark.read.csv(\"file:////C:\\\\Users\\\\dev30\\\\Documents\\\\pyspark_practice\\\\sales.csv\", header=True, mode=\"DROPMALFORMED\")\\n\\n#sales.show()\\n\\nsales.repartition(200, col(\"product_id\")).write.parquet(\"file:////C:\\\\Users\\\\dev30\\\\Documents\\\\pyspark_practice\\\\sales_parquet\", mode=\"overwrite\")\\n\\nsellers = spark.read.csv(\"file:////C:\\\\Users\\\\dev30\\\\Documents\\\\pyspark_practice\\\\sellers.csv\", header=True, mode=\"DROPMALFORMED\")\\n\\n#sellers.show()\\n\\nsellers.write.parquet(\"file:////C:\\\\Users\\\\dev30\\\\Documents\\\\pyspark_practice\\\\sellers_parquet\", mode=\"overwrite\")\\n\\nprint(\"====done====\")\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "spark = SparkSession.builder.master(\"local\")\\\n",
    ".config(\"spark.sql.autoBroadcastJoinThreshold\", -1)\\\n",
    ".appName(\"Exercise1\").getOrCreate()\n",
    "\n",
    "products = spark.read.csv(\"file:////C:\\\\Users\\\\dev30\\\\Documents\\\\pyspark_practice\\\\products.csv\", header=True, mode=\"DROPMALFORMED\")\n",
    "\n",
    "#products.show()\n",
    "\n",
    "products.write.parquet(\"file:////C:\\\\Users\\\\dev30\\\\Documents\\\\pyspark_practice\\\\products_parquet\", mode=\"overwrite\")\n",
    "\n",
    "sales = spark.read.csv(\"file:////C:\\\\Users\\\\dev30\\\\Documents\\\\pyspark_practice\\\\sales.csv\", header=True, mode=\"DROPMALFORMED\")\n",
    "\n",
    "#sales.show()\n",
    "\n",
    "sales.repartition(200, col(\"product_id\")).write.parquet(\"file:////C:\\\\Users\\\\dev30\\\\Documents\\\\pyspark_practice\\\\sales_parquet\", mode=\"overwrite\")\n",
    "\n",
    "sellers = spark.read.csv(\"file:////C:\\\\Users\\\\dev30\\\\Documents\\\\pyspark_practice\\\\sellers.csv\", header=True, mode=\"DROPMALFORMED\")\n",
    "\n",
    "#sellers.show()\n",
    "\n",
    "sellers.write.parquet(\"file:////C:\\\\Users\\\\dev30\\\\Documents\\\\pyspark_practice\\\\sellers_parquet\", mode=\"overwrite\")\n",
    "\n",
    "print(\"====done====\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37d7014",
   "metadata": {},
   "source": [
    "# Find out how many orders, how many products and how many sellers are in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e945d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Orders: 20000040\n",
      "Number of sellers: 10\n",
      "Number of products: 75000000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "#   Initialize the Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "    .config(\"spark.executor.memory\", \"500mb\") \\\n",
    "    .appName(\"Exercise1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#   Read the source tables in Parquet format\n",
    "products_table = spark.read.parquet(\"file:////C:\\\\Users\\\\dev30\\\\Documents\\\\pyspark_practice\\\\products_parquet\")\n",
    "sales_table = spark.read.parquet(\"file:////C:\\\\Users\\\\dev30\\\\Documents\\\\pyspark_practice\\\\sales_parquet\")\n",
    "sellers_table = spark.read.parquet(\"file:////C:\\\\Users\\\\dev30\\\\Documents\\\\pyspark_practice\\\\sellers_parquet\")\n",
    "\n",
    "#   Print the number of orders\n",
    "print(\"Number of Orders: {}\".format(sales_table.count()))\n",
    "\n",
    "#   Print the number of sellers\n",
    "print(\"Number of sellers: {}\".format(sellers_table.count()))\n",
    "\n",
    "#   Print the number of products\n",
    "print(\"Number of products: {}\".format(products_table.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dde12ad",
   "metadata": {},
   "source": [
    "# How many products have been sold at least once? Which is the product contained in more orders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1c72c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of products sold at least once\n",
      "+--------------------------+\n",
      "|count(DISTINCT product_id)|\n",
      "+--------------------------+\n",
      "|                    993299|\n",
      "+--------------------------+\n",
      "\n",
      "Product present in more orders\n",
      "+----------+--------+\n",
      "|product_id|     cnt|\n",
      "+----------+--------+\n",
      "|         0|19000000|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#   Output how many products have been actually sold at least once\n",
    "print(\"Number of products sold at least once\")\n",
    "sales_table.agg(countDistinct(col(\"product_id\"))).show()\n",
    "\n",
    "#   Output which is the product that has been sold in more orders\n",
    "print(\"Product present in more orders\")\n",
    "sales_table.groupBy(col(\"product_id\")).agg(count(\"*\").alias(\"cnt\")).orderBy(col(\"cnt\").desc()).limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb2dfb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------+\n",
      "|      date|distinct_products_sold|\n",
      "+----------+----------------------+\n",
      "|2020-07-04|                100294|\n",
      "|2020-07-03|                100224|\n",
      "|2020-07-10|                100218|\n",
      "|2020-07-08|                100048|\n",
      "|2020-07-05|                 99991|\n",
      "|2020-07-06|                 99869|\n",
      "|2020-07-09|                 99801|\n",
      "|2020-07-02|                 99768|\n",
      "|2020-07-01|                 99755|\n",
      "|2020-07-07|                 99453|\n",
      "+----------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_table.groupby(col(\"date\")).agg(countDistinct(col(\"product_id\")).alias(\"distinct_products_sold\")).orderBy(\n",
    "    col(\"distinct_products_sold\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac273887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|avg((price * num_pieces_sold))|\n",
      "+------------------------------+\n",
      "|             1245.923923502153|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#   What is the average revenue of the orders?\n",
    "# Do the join and print the results\n",
    "sales_table.join(products_table, sales_table[\"product_id\"] == products_table[\"product_id\"], \"inner\").agg(avg(products_table[\"price\"] * sales_table[\"num_pieces_sold\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61fa7b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|avg((price * num_pieces_sold))|\n",
      "+------------------------------+\n",
      "|             1245.923923502153|\n",
      "+------------------------------+\n",
      "\n",
      "None\n",
      "Ok\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Create the Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "    .config(\"spark.executor.memory\", \"500mb\") \\\n",
    "    .appName(\"Exercise1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the source tables\n",
    "products_table = spark.read.parquet(\"file:////C:\\\\Users\\\\dev30\\\\Documents\\\\pyspark_practice\\\\products_parquet\")\n",
    "sales_table = spark.read.parquet(\"file:////C:\\\\Users\\\\dev30\\\\Documents\\\\pyspark_practice\\\\sales_parquet\")\n",
    "sellers_table = spark.read.parquet(\"file:////C:\\\\Users\\\\dev30\\\\Documents\\\\pyspark_practice\\\\sellers_parquet\")\n",
    "\n",
    "# Step 1 - Check and select the skewed keys \n",
    "# In this case we are retrieving the top 100 keys: these will be the only salted keys.\n",
    "results = sales_table.groupby(sales_table[\"product_id\"]).count().sort(col(\"count\").desc()).limit(100).collect()\n",
    "\n",
    "# Step 2 - What we want to do is:\n",
    "#  a. Duplicate the entries that we have in the dimension table for the most common products, e.g.\n",
    "#       product_0 will become: product_0-1, product_0-2, product_0-3 and so on\n",
    "#  b. On the sales table, we are going to replace \"product_0\" with a random duplicate (e.g. some of them \n",
    "#     will be replaced with product_0-1, others with product_0-2, etc.)\n",
    "# Using the new \"salted\" key will unskew the join\n",
    "\n",
    "# Let's create a dataset to do the trick\n",
    "REPLICATION_FACTOR = 101\n",
    "l = []\n",
    "replicated_products = []\n",
    "for _r in results:\n",
    "    replicated_products.append(_r[\"product_id\"])\n",
    "    for _rep in range(0, REPLICATION_FACTOR):\n",
    "        l.append((_r[\"product_id\"], _rep))\n",
    "rdd = spark.sparkContext.parallelize(l)\n",
    "replicated_df = rdd.map(lambda x: Row(product_id=x[0], replication=int(x[1])))\n",
    "replicated_df = spark.createDataFrame(replicated_df)\n",
    "\n",
    "#   Step 3: Generate the salted key\n",
    "products_table = products_table.join(broadcast(replicated_df),\n",
    "                                     products_table[\"product_id\"] == replicated_df[\"product_id\"], \"left\"). \\\n",
    "    withColumn(\"salted_join_key\", when(replicated_df[\"replication\"].isNull(), products_table[\"product_id\"]).otherwise(\n",
    "    concat(replicated_df[\"product_id\"], lit(\"-\"), replicated_df[\"replication\"])))\n",
    "\n",
    "sales_table = sales_table.withColumn(\"salted_join_key\", when(sales_table[\"product_id\"].isin(replicated_products),\n",
    "                                                             concat(sales_table[\"product_id\"], lit(\"-\"),\n",
    "                                                                    round(rand() * (REPLICATION_FACTOR - 1), 0).cast(\n",
    "                                                                        IntegerType()))).otherwise(\n",
    "    sales_table[\"product_id\"]))\n",
    "\n",
    "#   Step 4: Finally let's do the join\n",
    "print(sales_table.join(products_table, sales_table[\"salted_join_key\"] == products_table[\"salted_join_key\"],\n",
    "                       \"inner\").\n",
    "      agg(avg(products_table[\"price\"] * sales_table[\"num_pieces_sold\"])).show())\n",
    "\n",
    "print(\"Ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dee22168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|seller_id|          avg(ratio)|\n",
      "+---------+--------------------+\n",
      "|        7|2.593509710759785...|\n",
      "|        3|1.627171975063155...|\n",
      "|        8|9.231037778437675E-5|\n",
      "|        0|2.019736770553698E-5|\n",
      "|        5|4.206816272236925...|\n",
      "|        6|4.793659021438702E-5|\n",
      "|        9|3.829272988800400...|\n",
      "|        1|1.962610829274823E-4|\n",
      "|        4|3.300641353602719...|\n",
      "|        2|6.689119886281191E-5|\n",
      "+---------+--------------------+\n",
      "\n",
      "None\n",
      "+---------+--------------------+\n",
      "|seller_id|          avg(ratio)|\n",
      "+---------+--------------------+\n",
      "|        7|2.593509710759747...|\n",
      "|        3|1.627171975063136...|\n",
      "|        8|9.231037778437045E-5|\n",
      "|        0|2.019736770525865...|\n",
      "|        5|4.206816272236251E-5|\n",
      "|        6| 4.79365902143776E-5|\n",
      "|        9| 3.82927298880055E-5|\n",
      "|        1|1.962610829274634...|\n",
      "|        4|3.300641353603240...|\n",
      "|        2|6.689119886277543E-5|\n",
      "+---------+--------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#   For each seller find the average % of the target amount brought by each order\n",
    "\n",
    "#   Wrong way to do this - Skewed \n",
    "#   (Note that Spark will probably broadcast the table anyway, unless we forbid it throug the configuration paramters)\n",
    "print(sales_table.join(sellers_table, sales_table[\"seller_id\"] == sellers_table[\"seller_id\"], \"inner\").withColumn(\n",
    "    \"ratio\", sales_table[\"num_pieces_sold\"]/sellers_table[\"daily_target\"]\n",
    ").groupBy(sales_table[\"seller_id\"]).agg(avg(\"ratio\")).show())\n",
    "\n",
    "#   Correct way through broarcasting\n",
    "print(sales_table.join(broadcast(sellers_table), sales_table[\"seller_id\"] == sellers_table[\"seller_id\"], \"inner\").withColumn(\n",
    "    \"ratio\", sales_table[\"num_pieces_sold\"]/sellers_table[\"daily_target\"]\n",
    ").groupBy(sales_table[\"seller_id\"]).agg(avg(\"ratio\")).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6a5663b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------------+\n",
      "|product_id|seller_id|        type|\n",
      "+----------+---------+------------+\n",
      "|  25078796|        4|Least Seller|\n",
      "|  22113759|        7|Least Seller|\n",
      "|   3384033|        1|Least Seller|\n",
      "|  57452470|        2|Least Seller|\n",
      "|  63583705|        8|Least Seller|\n",
      "|  46764734|        5|Least Seller|\n",
      "|  24992315|        5|Least Seller|\n",
      "|   5342885|        7|Least Seller|\n",
      "|  57356373|        6|Least Seller|\n",
      "|  19581081|        1|Least Seller|\n",
      "|  70135114|        4|Least Seller|\n",
      "|  44179664|        7|Least Seller|\n",
      "|  74608973|        7|Least Seller|\n",
      "|  19657520|        8|Least Seller|\n",
      "|  54810407|        7|Least Seller|\n",
      "|  55049289|        6|Least Seller|\n",
      "|  36289301|        6|Least Seller|\n",
      "|   3147422|        1|Least Seller|\n",
      "|  28365122|        9|Least Seller|\n",
      "|  49196172|        4|Least Seller|\n",
      "+----------+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+---------+--------------------+\n",
      "|product_id|seller_id|                type|\n",
      "+----------+---------+--------------------+\n",
      "|         0|        0|Only seller or mu...|\n",
      "+----------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "# Calcuate the number of pieces sold by each seller for each product\n",
    "sales_table = sales_table.groupby(col(\"product_id\"), col(\"seller_id\")). \\\n",
    "    agg(sum(\"num_pieces_sold\").alias(\"num_pieces_sold\"))\n",
    "\n",
    "# Create the window functions, one will sort ascending the other one descending. Partition by the product_id\n",
    "# and sort by the pieces sold\n",
    "window_desc = Window.partitionBy(col(\"product_id\")).orderBy(col(\"num_pieces_sold\").desc())\n",
    "window_asc = Window.partitionBy(col(\"product_id\")).orderBy(col(\"num_pieces_sold\").asc())\n",
    "\n",
    "# Create a Dense Rank (to avoid holes)\n",
    "sales_table = sales_table.withColumn(\"rank_asc\", dense_rank().over(window_asc)). \\\n",
    "    withColumn(\"rank_desc\", dense_rank().over(window_desc))\n",
    "\n",
    "# Get products that only have one row OR the products in which multiple sellers sold the same amount\n",
    "# (i.e. all the employees that ever sold the product, sold the same exact amount)\n",
    "single_seller = sales_table.where(col(\"rank_asc\") == col(\"rank_desc\")).select(\n",
    "    col(\"product_id\").alias(\"single_seller_product_id\"), col(\"seller_id\").alias(\"single_seller_seller_id\"),\n",
    "    lit(\"Only seller or multiple sellers with the same results\").alias(\"type\")\n",
    ")\n",
    "\n",
    "# Get the second top sellers\n",
    "second_seller = sales_table.where(col(\"rank_desc\") == 2).select(\n",
    "    col(\"product_id\").alias(\"second_seller_product_id\"), col(\"seller_id\").alias(\"second_seller_seller_id\"),\n",
    "    lit(\"Second top seller\").alias(\"type\")\n",
    ")\n",
    "\n",
    "# Get the least sellers and exclude those rows that are already included in the first piece\n",
    "# We also exclude the \"second top sellers\" that are also \"least sellers\"\n",
    "least_seller = sales_table.where(col(\"rank_asc\") == 1).select(\n",
    "    col(\"product_id\"), col(\"seller_id\"),\n",
    "    lit(\"Least Seller\").alias(\"type\")\n",
    ").join(single_seller, (sales_table[\"seller_id\"] == single_seller[\"single_seller_seller_id\"]) & (\n",
    "        sales_table[\"product_id\"] == single_seller[\"single_seller_product_id\"]), \"left_anti\"). \\\n",
    "    join(second_seller, (sales_table[\"seller_id\"] == second_seller[\"second_seller_seller_id\"]) & (\n",
    "        sales_table[\"product_id\"] == second_seller[\"second_seller_product_id\"]), \"left_anti\")\n",
    "\n",
    "# Union all the pieces\n",
    "union_table = least_seller.select(\n",
    "    col(\"product_id\"),\n",
    "    col(\"seller_id\"),\n",
    "    col(\"type\")\n",
    ").union(second_seller.select(\n",
    "    col(\"second_seller_product_id\").alias(\"product_id\"),\n",
    "    col(\"second_seller_seller_id\").alias(\"seller_id\"),\n",
    "    col(\"type\")\n",
    ")).union(single_seller.select(\n",
    "    col(\"single_seller_product_id\").alias(\"product_id\"),\n",
    "    col(\"single_seller_seller_id\").alias(\"seller_id\"),\n",
    "    col(\"type\")\n",
    "))\n",
    "union_table.show()\n",
    "\n",
    "# Which are the second top seller and least seller of product 0?\n",
    "union_table.where(col(\"product_id\") == 0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1420f225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+\n",
      "|hashed_bill|cnt|\n",
      "+-----------+---+\n",
      "+-----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Row, Window\n",
    "from pyspark.sql.types import IntegerType\n",
    "import hashlib\n",
    "\n",
    "#   Init spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .appName(\"Exercise1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#   Load source data\n",
    "products_table = spark.read.parquet(\"file:////C:\\\\Users\\\\dev30\\\\Documents\\\\pyspark_practice\\\\products_parquet\")\n",
    "sales_table = spark.read.parquet(\"file:////C:\\\\Users\\\\dev30\\\\Documents\\\\pyspark_practice\\\\sales_parquet\")\n",
    "sellers_table = spark.read.parquet(\"file:////C:\\\\Users\\\\dev30\\\\Documents\\\\pyspark_practice\\\\sellers_parquet\")\n",
    "\n",
    "#   Define the UDF function\n",
    "def algo(order_id, bill_text):\n",
    "    #   If number is even\n",
    "    ret = bill_text.encode(\"utf-8\")\n",
    "    if int(order_id) % 2 == 0:\n",
    "        #   Count number of 'A'\n",
    "        cnt_A = bill_text.count(\"A\")\n",
    "        for _c in range(0, cnt_A):\n",
    "            ret = hashlib.md5(ret).hexdigest().encode(\"utf-8\")\n",
    "        ret = ret.decode('utf-8')\n",
    "    else:\n",
    "        ret = hashlib.sha256(ret).hexdigest()\n",
    "    return ret\n",
    "\n",
    "#   Register the UDF function.\n",
    "algo_udf = spark.udf.register(\"algo\", algo)\n",
    "\n",
    "#   Use the `algo_udf` to apply the aglorithm and then check if there is any duplicate hash in the table\n",
    "sales_table.withColumn(\"hashed_bill\", algo_udf(col(\"order_id\"), col(\"bill_raw_text\")))\\\n",
    "    .groupby(col(\"hashed_bill\")).agg(count(\"*\").alias(\"cnt\")).where(col(\"cnt\") > 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddc88b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f8abec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c11174f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd92814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea74102",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
